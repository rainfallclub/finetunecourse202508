{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 81,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.037037037037037035,
      "grad_norm": 3.234429359436035,
      "learning_rate": 0.0003,
      "loss": 4.0687,
      "step": 1
    },
    {
      "epoch": 0.07407407407407407,
      "grad_norm": 4.907257080078125,
      "learning_rate": 0.00029629629629629624,
      "loss": 4.9741,
      "step": 2
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 2.9472224712371826,
      "learning_rate": 0.00029259259259259256,
      "loss": 3.0876,
      "step": 3
    },
    {
      "epoch": 0.14814814814814814,
      "grad_norm": 2.4695611000061035,
      "learning_rate": 0.0002888888888888888,
      "loss": 3.13,
      "step": 4
    },
    {
      "epoch": 0.18518518518518517,
      "grad_norm": 3.522982597351074,
      "learning_rate": 0.00028518518518518514,
      "loss": 2.6723,
      "step": 5
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 2.9245808124542236,
      "learning_rate": 0.00028148148148148146,
      "loss": 2.3369,
      "step": 6
    },
    {
      "epoch": 0.25925925925925924,
      "grad_norm": 3.042451858520508,
      "learning_rate": 0.0002777777777777778,
      "loss": 3.1393,
      "step": 7
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 4.1232380867004395,
      "learning_rate": 0.00027407407407407404,
      "loss": 2.8429,
      "step": 8
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 2.472914457321167,
      "learning_rate": 0.00027037037037037036,
      "loss": 2.205,
      "step": 9
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 5.246352195739746,
      "learning_rate": 0.0002666666666666666,
      "loss": 2.8664,
      "step": 10
    },
    {
      "epoch": 0.4074074074074074,
      "grad_norm": 2.8053715229034424,
      "learning_rate": 0.00026296296296296294,
      "loss": 1.8325,
      "step": 11
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 2.5458226203918457,
      "learning_rate": 0.0002592592592592592,
      "loss": 1.8237,
      "step": 12
    },
    {
      "epoch": 0.48148148148148145,
      "grad_norm": 3.7210803031921387,
      "learning_rate": 0.00025555555555555553,
      "loss": 2.008,
      "step": 13
    },
    {
      "epoch": 0.5185185185185185,
      "grad_norm": 2.5437264442443848,
      "learning_rate": 0.0002518518518518518,
      "loss": 1.5032,
      "step": 14
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 2.376584053039551,
      "learning_rate": 0.0002481481481481481,
      "loss": 1.8189,
      "step": 15
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 2.9730381965637207,
      "learning_rate": 0.00024444444444444443,
      "loss": 1.7752,
      "step": 16
    },
    {
      "epoch": 0.6296296296296297,
      "grad_norm": 2.9175941944122314,
      "learning_rate": 0.00024074074074074072,
      "loss": 1.6236,
      "step": 17
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 3.315861940383911,
      "learning_rate": 0.000237037037037037,
      "loss": 1.4364,
      "step": 18
    },
    {
      "epoch": 0.7037037037037037,
      "grad_norm": 3.1952381134033203,
      "learning_rate": 0.0002333333333333333,
      "loss": 1.6022,
      "step": 19
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 3.9369826316833496,
      "learning_rate": 0.0002296296296296296,
      "loss": 1.9785,
      "step": 20
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 3.8875927925109863,
      "learning_rate": 0.00022592592592592591,
      "loss": 1.8594,
      "step": 21
    },
    {
      "epoch": 0.8148148148148148,
      "grad_norm": 2.885801076889038,
      "learning_rate": 0.00022222222222222218,
      "loss": 1.354,
      "step": 22
    },
    {
      "epoch": 0.8518518518518519,
      "grad_norm": 4.123275279998779,
      "learning_rate": 0.0002185185185185185,
      "loss": 1.6262,
      "step": 23
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 4.067672252655029,
      "learning_rate": 0.0002148148148148148,
      "loss": 1.4071,
      "step": 24
    },
    {
      "epoch": 0.9259259259259259,
      "grad_norm": 2.3138809204101562,
      "learning_rate": 0.0002111111111111111,
      "loss": 1.0578,
      "step": 25
    },
    {
      "epoch": 0.9629629629629629,
      "grad_norm": 2.3387184143066406,
      "learning_rate": 0.00020740740740740737,
      "loss": 1.4689,
      "step": 26
    },
    {
      "epoch": 1.0,
      "grad_norm": 3.9884164333343506,
      "learning_rate": 0.0002037037037037037,
      "loss": 1.627,
      "step": 27
    },
    {
      "epoch": 1.037037037037037,
      "grad_norm": 3.116487503051758,
      "learning_rate": 0.00019999999999999998,
      "loss": 0.9722,
      "step": 28
    },
    {
      "epoch": 1.074074074074074,
      "grad_norm": 4.3855390548706055,
      "learning_rate": 0.00019629629629629627,
      "loss": 1.1424,
      "step": 29
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 3.061552047729492,
      "learning_rate": 0.00019259259259259257,
      "loss": 1.0501,
      "step": 30
    },
    {
      "epoch": 1.1481481481481481,
      "grad_norm": 2.6639952659606934,
      "learning_rate": 0.00018888888888888888,
      "loss": 0.9242,
      "step": 31
    },
    {
      "epoch": 1.1851851851851851,
      "grad_norm": 1.9751654863357544,
      "learning_rate": 0.00018518518518518515,
      "loss": 1.2251,
      "step": 32
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 3.469794511795044,
      "learning_rate": 0.00018148148148148147,
      "loss": 0.9398,
      "step": 33
    },
    {
      "epoch": 1.2592592592592593,
      "grad_norm": 2.121403694152832,
      "learning_rate": 0.00017777777777777776,
      "loss": 0.9487,
      "step": 34
    },
    {
      "epoch": 1.2962962962962963,
      "grad_norm": 1.9488054513931274,
      "learning_rate": 0.00017407407407407408,
      "loss": 0.9142,
      "step": 35
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 2.249976634979248,
      "learning_rate": 0.00017037037037037034,
      "loss": 0.8755,
      "step": 36
    },
    {
      "epoch": 1.3703703703703702,
      "grad_norm": 2.643829822540283,
      "learning_rate": 0.00016666666666666666,
      "loss": 0.9823,
      "step": 37
    },
    {
      "epoch": 1.4074074074074074,
      "grad_norm": 1.851925015449524,
      "learning_rate": 0.00016296296296296295,
      "loss": 0.9843,
      "step": 38
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 2.151524305343628,
      "learning_rate": 0.00015925925925925924,
      "loss": 0.6463,
      "step": 39
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 1.967566967010498,
      "learning_rate": 0.00015555555555555554,
      "loss": 0.9405,
      "step": 40
    },
    {
      "epoch": 1.5185185185185186,
      "grad_norm": 4.049442768096924,
      "learning_rate": 0.00015185185185185185,
      "loss": 1.1496,
      "step": 41
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 2.963204860687256,
      "learning_rate": 0.00014814814814814812,
      "loss": 1.015,
      "step": 42
    },
    {
      "epoch": 1.5925925925925926,
      "grad_norm": 2.499232769012451,
      "learning_rate": 0.0001444444444444444,
      "loss": 1.0168,
      "step": 43
    },
    {
      "epoch": 1.6296296296296298,
      "grad_norm": 2.146860122680664,
      "learning_rate": 0.00014074074074074073,
      "loss": 0.9202,
      "step": 44
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 2.7817299365997314,
      "learning_rate": 0.00013703703703703702,
      "loss": 1.2369,
      "step": 45
    },
    {
      "epoch": 1.7037037037037037,
      "grad_norm": 3.500967502593994,
      "learning_rate": 0.0001333333333333333,
      "loss": 1.4172,
      "step": 46
    },
    {
      "epoch": 1.7407407407407407,
      "grad_norm": 3.0669941902160645,
      "learning_rate": 0.0001296296296296296,
      "loss": 0.9743,
      "step": 47
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 1.9838148355484009,
      "learning_rate": 0.0001259259259259259,
      "loss": 0.7412,
      "step": 48
    },
    {
      "epoch": 1.8148148148148149,
      "grad_norm": 2.559870719909668,
      "learning_rate": 0.00012222222222222221,
      "loss": 1.1723,
      "step": 49
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 2.7703728675842285,
      "learning_rate": 0.0001185185185185185,
      "loss": 1.1552,
      "step": 50
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 2.596987247467041,
      "learning_rate": 0.0001148148148148148,
      "loss": 1.0581,
      "step": 51
    },
    {
      "epoch": 1.925925925925926,
      "grad_norm": 3.21330189704895,
      "learning_rate": 0.00011111111111111109,
      "loss": 0.807,
      "step": 52
    },
    {
      "epoch": 1.9629629629629628,
      "grad_norm": 2.165252447128296,
      "learning_rate": 0.0001074074074074074,
      "loss": 1.0337,
      "step": 53
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.8766493797302246,
      "learning_rate": 0.00010370370370370369,
      "loss": 1.0704,
      "step": 54
    },
    {
      "epoch": 2.037037037037037,
      "grad_norm": 2.104566812515259,
      "learning_rate": 9.999999999999999e-05,
      "loss": 1.0278,
      "step": 55
    },
    {
      "epoch": 2.074074074074074,
      "grad_norm": 2.3399267196655273,
      "learning_rate": 9.629629629629628e-05,
      "loss": 1.0366,
      "step": 56
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 2.0958943367004395,
      "learning_rate": 9.259259259259257e-05,
      "loss": 0.6165,
      "step": 57
    },
    {
      "epoch": 2.148148148148148,
      "grad_norm": 2.484762191772461,
      "learning_rate": 8.888888888888888e-05,
      "loss": 0.7774,
      "step": 58
    },
    {
      "epoch": 2.185185185185185,
      "grad_norm": 1.931637167930603,
      "learning_rate": 8.518518518518517e-05,
      "loss": 0.8422,
      "step": 59
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 2.406461477279663,
      "learning_rate": 8.148148148148148e-05,
      "loss": 0.5338,
      "step": 60
    },
    {
      "epoch": 2.259259259259259,
      "grad_norm": 3.3366549015045166,
      "learning_rate": 7.777777777777777e-05,
      "loss": 1.1756,
      "step": 61
    },
    {
      "epoch": 2.2962962962962963,
      "grad_norm": 2.7054896354675293,
      "learning_rate": 7.407407407407406e-05,
      "loss": 1.1586,
      "step": 62
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 2.1630074977874756,
      "learning_rate": 7.037037037037036e-05,
      "loss": 0.7109,
      "step": 63
    },
    {
      "epoch": 2.3703703703703702,
      "grad_norm": 2.4227488040924072,
      "learning_rate": 6.666666666666666e-05,
      "loss": 1.1296,
      "step": 64
    },
    {
      "epoch": 2.4074074074074074,
      "grad_norm": 3.165588855743408,
      "learning_rate": 6.296296296296295e-05,
      "loss": 0.5123,
      "step": 65
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 2.209280014038086,
      "learning_rate": 5.925925925925925e-05,
      "loss": 0.5406,
      "step": 66
    },
    {
      "epoch": 2.4814814814814814,
      "grad_norm": 2.219849109649658,
      "learning_rate": 5.5555555555555545e-05,
      "loss": 1.0261,
      "step": 67
    },
    {
      "epoch": 2.5185185185185186,
      "grad_norm": 2.701632022857666,
      "learning_rate": 5.185185185185184e-05,
      "loss": 0.7364,
      "step": 68
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 2.485147476196289,
      "learning_rate": 4.814814814814814e-05,
      "loss": 0.5746,
      "step": 69
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 2.0058908462524414,
      "learning_rate": 4.444444444444444e-05,
      "loss": 0.7881,
      "step": 70
    },
    {
      "epoch": 2.6296296296296298,
      "grad_norm": 3.5953526496887207,
      "learning_rate": 4.074074074074074e-05,
      "loss": 1.0063,
      "step": 71
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 3.349956750869751,
      "learning_rate": 3.703703703703703e-05,
      "loss": 0.7527,
      "step": 72
    },
    {
      "epoch": 2.7037037037037037,
      "grad_norm": 3.3173553943634033,
      "learning_rate": 3.333333333333333e-05,
      "loss": 1.1233,
      "step": 73
    },
    {
      "epoch": 2.7407407407407405,
      "grad_norm": 2.030414581298828,
      "learning_rate": 2.9629629629629627e-05,
      "loss": 0.6085,
      "step": 74
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 2.3278281688690186,
      "learning_rate": 2.592592592592592e-05,
      "loss": 0.5218,
      "step": 75
    },
    {
      "epoch": 2.814814814814815,
      "grad_norm": 2.59358286857605,
      "learning_rate": 2.222222222222222e-05,
      "loss": 0.7071,
      "step": 76
    },
    {
      "epoch": 2.851851851851852,
      "grad_norm": 2.3504743576049805,
      "learning_rate": 1.8518518518518515e-05,
      "loss": 0.6618,
      "step": 77
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 2.962785482406616,
      "learning_rate": 1.4814814814814813e-05,
      "loss": 0.7462,
      "step": 78
    },
    {
      "epoch": 2.925925925925926,
      "grad_norm": 2.341207504272461,
      "learning_rate": 1.111111111111111e-05,
      "loss": 0.6315,
      "step": 79
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 3.932955741882324,
      "learning_rate": 7.407407407407407e-06,
      "loss": 0.6787,
      "step": 80
    },
    {
      "epoch": 3.0,
      "grad_norm": 2.7332699298858643,
      "learning_rate": 3.7037037037037033e-06,
      "loss": 0.7481,
      "step": 81
    }
  ],
  "logging_steps": 1,
  "max_steps": 81,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 52193577664512.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
